# Google Driveçµ±åˆè«–æ–‡ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ  å®Ÿè£…è¨ˆç”»

## ğŸ¯ æ¦‚è¦

æœ¬æ–‡æ›¸ã¯ã€ç¾åœ¨å‹•ä½œä¸­ã®PDF Math Translate Chrome Extensionã«å½±éŸ¿ã‚’ä¸ãˆã‚‹ã“ã¨ãªãã€Google Driveçµ±åˆè«–æ–‡ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’æ®µéšçš„ã«å®Ÿè£…ã™ã‚‹ãŸã‚ã®è©³ç´°è¨ˆç”»ã§ã™ã€‚

## âš ï¸ é‡è¦ãªåˆ¶ç´„

- **ç¾åœ¨ã®æ‹¡å¼µæ©Ÿèƒ½ã®æ©Ÿèƒ½ã‚’ç¶­æŒ**: æ—¢å­˜ã®ç¿»è¨³æ©Ÿèƒ½ã¯ä¸€åˆ‡å¤‰æ›´ã—ãªã„
- **æ®µéšçš„å®Ÿè£…**: æ–°æ©Ÿèƒ½ã¯ç‹¬ç«‹ã—ãŸãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ã—ã¦å®Ÿè£…
- **å¾Œæ–¹äº’æ›æ€§**: æ—¢å­˜ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ç ´ç¶»ã•ã›ãªã„
- **ã‚ªãƒ—ãƒˆã‚¤ãƒ³æ–¹å¼**: Google Driveæ©Ÿèƒ½ã¯è¨­å®šã§æœ‰åŠ¹åŒ–

## ğŸ—ï¸ ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 
```
Google Drive/Research_Papers/
â”œâ”€â”€ papers/                          # å®Ÿéš›ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆä¸€ç®‡æ‰€é›†ä¸­ï¼‰
â”œâ”€â”€ metadata/
â”‚   â”œâ”€â”€ papers_index.json           # è«–æ–‡ç®¡ç†ç”¨JSON
â”‚   â””â”€â”€ categories_config.json      # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç®¡ç†ç”¨JSON
â””â”€â”€ shortcuts/                      # ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆã«ã‚ˆã‚‹åˆ†é¡
    â”œâ”€â”€ ML_AI/
    â”œâ”€â”€ NLP/
    â””â”€â”€ Computer_Vision/
```

### ãƒ‡ãƒ¼ã‚¿æ§‹é€ 

#### papers_index.json
```json
{
  "papers": {
    "2506.06751": {
      "arxiv_id": "2506.06751",
      "title": "Large Language Models for Mathematical Reasoning",
      "abstract": "In this paper, we investigate...",
      "authors": ["John Doe", "Jane Smith"],
      "arxiv_url": "https://arxiv.org/abs/2506.06751",
      "local_file": "papers/2506.06751-dual.pdf",
      "drive_file_id": "1ABC...XYZ",
      "llm_tags": ["mathematics", "reasoning", "transformer"],
      "confidence_scores": {
        "ML_AI": 0.85,
        "Mathematics": 0.92,
        "NLP": 0.73
      },
      "primary_category": "Mathematics",
      "secondary_categories": ["ML_AI", "NLP"],
      "shortcut_paths": [
        "shortcuts/Mathematics/Reasoning/2506.06751-dual.pdf",
        "shortcuts/ML_AI/LLM/2506.06751-dual.pdf"
      ],
      "translation_info": {
        "service": "anthropic:claude-3-5-sonnet-20241022",
        "target_language": "ja",
        "format": "dual",
        "translated_at": "2025-06-12T10:30:00Z"
      },
      "metadata": {
        "added_at": "2025-06-12T10:25:00Z",
        "last_updated": "2025-06-12T10:35:00Z",
        "source": "arxiv_api",
        "abstract_source": "arxiv_api"
      }
    }
  },
  "statistics": {
    "total_papers": 1247,
    "last_updated": "2025-06-12T10:35:00Z",
    "categories_count": {
      "ML_AI": 456,
      "NLP": 332,
      "Computer_Vision": 287,
      "Mathematics": 172
    }
  }
}
```

#### categories_config.json
```json
{
  "version": "2.0",
  "last_updated": "2025-06-12T10:00:00Z",
  "categories": {
    "ML_AI": {
      "name": "æ©Ÿæ¢°å­¦ç¿’ãƒ»AI",
      "keywords": ["machine learning", "deep learning", "neural network", "AI"],
      "subcategories": {
        "Deep_Learning": {
          "keywords": ["deep learning", "neural network", "CNN", "RNN"]
        },
        "Reinforcement_Learning": {
          "keywords": ["reinforcement learning", "RL", "policy", "reward"]
        },
        "LLM": {
          "keywords": ["large language model", "transformer", "GPT", "BERT"]
        }
      }
    },
    "NLP": {
      "name": "è‡ªç„¶è¨€èªå‡¦ç†",
      "keywords": ["natural language", "text", "language model", "NLP"],
      "subcategories": {
        "Text_Generation": {
          "keywords": ["text generation", "language generation"]
        },
        "Machine_Translation": {
          "keywords": ["translation", "machine translation", "MT"]
        }
      }
    }
  },
  "classification_rules": {
    "confidence_threshold": 0.7,
    "multi_category_limit": 3,
    "primary_category_min_confidence": 0.8
  }
}
```

## ğŸ“‹ å®Ÿè£…ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³

### Phase 1: åŸºç›¤ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«é–‹ç™º (Week 1-2)
**ç›®æ¨™**: æ—¢å­˜æ©Ÿèƒ½ã«å½±éŸ¿ã—ãªã„ç‹¬ç«‹ã—ãŸãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½œæˆ

#### 1.1 AbstractExtractor ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
**ãƒ•ã‚¡ã‚¤ãƒ«**: `pdf2zh/abstract_extractor.py`

```python
#!/usr/bin/env python3
"""
AbstractæŠ½å‡ºãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
arXiv API â†’ PDFæŠ½å‡ºã®2æ®µéšãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
"""

import arxiv
import pdfplumber
import re
import os
from typing import Optional, Dict, Any
from pathlib import Path

class AbstractExtractor:
    def __init__(self):
        self.arxiv_client = arxiv.Client()
    
    def extract_abstract(self, identifier: str, pdf_path: Optional[str] = None) -> Dict[str, Any]:
        """
        arXiv ID/PDF ãƒ‘ã‚¹ã‹ã‚‰Abstractã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        
        Args:
            identifier: arXiv ID (ä¾‹: "2506.06751")
            pdf_path: PDFãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰
            
        Returns:
            Dict containing title, abstract, authors, etc.
        """
        # Step 1: arXiv APIã§è©¦è¡Œ
        if self.is_arxiv_id(identifier):
            try:
                return self.extract_from_arxiv(identifier)
            except Exception as e:
                print(f"arXiv API failed: {e}")
        
        # Step 2: PDFã‹ã‚‰æŠ½å‡ºï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if pdf_path and os.path.exists(pdf_path):
            return self.extract_from_pdf(pdf_path)
        
        raise Exception("No valid source for abstract extraction")
    
    def is_arxiv_id(self, identifier: str) -> bool:
        """arXiv IDã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        # arXiv ID pattern: YYMM.NNNNN
        pattern = r'^\d{4}\.\d{4,5}$'
        return bool(re.match(pattern, identifier))
    
    def extract_from_arxiv(self, arxiv_id: str) -> Dict[str, Any]:
        """arXiv APIã‹ã‚‰æƒ…å ±å–å¾—"""
        search = arxiv.Search(id_list=[arxiv_id])
        results = list(self.arxiv_client.results(search))
        
        if not results:
            raise Exception(f"Paper not found: {arxiv_id}")
        
        paper = results[0]
        return {
            "title": paper.title,
            "abstract": paper.summary,
            "authors": [author.name for author in paper.authors],
            "arxiv_url": paper.entry_id,
            "published": paper.published.isoformat(),
            "source": "arxiv_api"
        }
    
    def extract_from_pdf(self, pdf_path: str) -> Dict[str, Any]:
        """PDFã‹ã‚‰AbstractæŠ½å‡º"""
        try:
            with pdfplumber.open(pdf_path) as pdf:
                first_page = pdf.pages[0]
                text = first_page.extract_text()
                
                # Abstractéƒ¨åˆ†ã‚’æ­£è¦è¡¨ç¾ã§æŠ½å‡º
                abstract_pattern = r'(?i)abstract[:\s]*\n?(.*?)(?=\n\s*(?:introduction|keywords|1\.|i\.|background))'
                match = re.search(abstract_pattern, text, re.DOTALL)
                
                if match:
                    abstract = match.group(1).strip()
                    title = self.extract_title_from_pdf(text)
                    
                    return {
                        "title": title,
                        "abstract": abstract,
                        "authors": [],  # PDFè§£æã§è‘—è€…æŠ½å‡ºã¯è¤‡é›‘
                        "source": "pdf_extraction"
                    }
        except Exception as e:
            print(f"PDF extraction failed: {e}")
        
        raise Exception("Could not extract abstract from PDF")
    
    def extract_title_from_pdf(self, text: str) -> str:
        """PDFã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        lines = text.split('\n')
        # é€šå¸¸ã€æœ€åˆã®æ•°è¡ŒãŒã‚¿ã‚¤ãƒˆãƒ«
        for line in lines[:5]:
            line = line.strip()
            if len(line) > 10 and not line.lower().startswith(('abstract', 'author', 'introduction')):
                return line
        return "Unknown Title"
```

#### 1.2 DriveManager ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
**ãƒ•ã‚¡ã‚¤ãƒ«**: `pdf2zh/drive_manager.py`

```python
#!/usr/bin/env python3
"""
Google Driveç®¡ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆä½œæˆã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€ãƒ•ã‚©ãƒ«ãƒ€ç®¡ç†
"""

import os
import json
from typing import Dict, Any, List, Optional
from pathlib import Path
from googleapiclient.discovery import build
from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from googleapiclient.http import MediaFileUpload

class DriveManager:
    def __init__(self, credentials_path: str, base_folder_name: str = "Research_Papers"):
        self.credentials_path = credentials_path
        self.base_folder_name = base_folder_name
        self.service = None
        self.base_folder_id = None
        self._initialize()
    
    def _initialize(self):
        """Google Drive APIåˆæœŸåŒ–"""
        try:
            self.service = self._authenticate()
            self.base_folder_id = self._get_or_create_base_folder()
            print(f"Google Drive initialized. Base folder ID: {self.base_folder_id}")
        except Exception as e:
            print(f"Google Drive initialization failed: {e}")
            self.service = None
    
    def _authenticate(self):
        """Google Drive APIèªè¨¼"""
        creds = None
        token_path = self.credentials_path.replace('.json', '_token.json')
        
        # æ—¢å­˜ã®ãƒˆãƒ¼ã‚¯ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        if os.path.exists(token_path):
            creds = Credentials.from_authorized_user_file(token_path)
        
        # èªè¨¼ãŒç„¡åŠ¹ãªå ´åˆã¯å†èªè¨¼
        if not creds or not creds.valid:
            if creds and creds.expired and creds.refresh_token:
                creds.refresh(Request())
            else:
                raise Exception("Invalid credentials. Please re-authenticate.")
        
        return build('drive', 'v3', credentials=creds)
    
    def _get_or_create_base_folder(self) -> str:
        """ãƒ™ãƒ¼ã‚¹ãƒ•ã‚©ãƒ«ãƒ€ã‚’å–å¾—ã¾ãŸã¯ä½œæˆ"""
        # æ—¢å­˜ãƒ•ã‚©ãƒ«ãƒ€ã‚’æ¤œç´¢
        results = self.service.files().list(
            q=f"name='{self.base_folder_name}' and mimeType='application/vnd.google-apps.folder'",
            fields="files(id, name)"
        ).execute()
        
        folders = results.get('files', [])
        if folders:
            return folders[0]['id']
        
        # ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
        folder_metadata = {
            'name': self.base_folder_name,
            'mimeType': 'application/vnd.google-apps.folder'
        }
        folder = self.service.files().create(body=folder_metadata, fields='id').execute()
        return folder.get('id')
    
    def upload_to_papers_folder(self, file_path: str, metadata: Dict[str, Any]) -> str:
        """papers/ãƒ•ã‚©ãƒ«ãƒ€ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
        papers_folder_id = self._get_or_create_subfolder('papers')
        
        file_metadata = {
            'name': os.path.basename(file_path),
            'parents': [papers_folder_id]
        }
        
        media = MediaFileUpload(file_path, resumable=True)
        file = self.service.files().create(
            body=file_metadata,
            media_body=media,
            fields='id'
        ).execute()
        
        return file.get('id')
    
    def create_shortcut(self, target_file_id: str, parent_folder_id: str, name: str) -> str:
        """Google Driveã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆä½œæˆ"""
        shortcut_metadata = {
            'name': name,
            'mimeType': 'application/vnd.google-apps.shortcut',
            'shortcutDetails': {
                'targetId': target_file_id
            },
            'parents': [parent_folder_id]
        }
        
        shortcut = self.service.files().create(
            body=shortcut_metadata,
            fields='id,shortcutDetails'
        ).execute()
        
        return shortcut.get('id')
    
    def _get_or_create_subfolder(self, folder_name: str, parent_id: Optional[str] = None) -> str:
        """ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ã‚’å–å¾—ã¾ãŸã¯ä½œæˆ"""
        if parent_id is None:
            parent_id = self.base_folder_id
        
        # æ—¢å­˜ãƒ•ã‚©ãƒ«ãƒ€ã‚’æ¤œç´¢
        results = self.service.files().list(
            q=f"name='{folder_name}' and parents in '{parent_id}' and mimeType='application/vnd.google-apps.folder'",
            fields="files(id, name)"
        ).execute()
        
        folders = results.get('files', [])
        if folders:
            return folders[0]['id']
        
        # ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ
        folder_metadata = {
            'name': folder_name,
            'mimeType': 'application/vnd.google-apps.folder',
            'parents': [parent_id]
        }
        folder = self.service.files().create(body=folder_metadata, fields='id').execute()
        return folder.get('id')
    
    def is_available(self) -> bool:
        """Google Driveæ©Ÿèƒ½ãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯"""
        return self.service is not None
```

#### 1.3 LLMClassifier ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
**ãƒ•ã‚¡ã‚¤ãƒ«**: `pdf2zh/llm_classifier.py`

```python
#!/usr/bin/env python3
"""
LLMè«–æ–‡åˆ†é¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
Claude/GPTã‚’ä½¿ç”¨ã—ãŸè«–æ–‡ã®è‡ªå‹•åˆ†é¡
"""

import json
import re
from typing import Dict, Any, List
from pdf2zh.translator import get_translator  # æ—¢å­˜ã®ç¿»è¨³æ©Ÿèƒ½ã‚’æ´»ç”¨

class LLMClassifier:
    def __init__(self, categories_config: Dict[str, Any], service: str = "anthropic:claude-3-5-sonnet-20241022"):
        self.categories = categories_config['categories']
        self.rules = categories_config['classification_rules']
        self.service = service
    
    def classify_paper(self, title: str, abstract: str) -> Dict[str, Any]:
        """LLMã‚’ä½¿ç”¨ã—ã¦è«–æ–‡ã‚’åˆ†é¡"""
        
        category_list = list(self.categories.keys())
        category_descriptions = {
            name: f"{config['name']}: {', '.join(config['keywords'][:5])}"
            for name, config in self.categories.items()
        }
        
        prompt = f"""ä»¥ä¸‹ã®è«–æ–‡ã‚’é©åˆ‡ãªã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡ã—ã¦ãã ã•ã„ï¼š

ã‚¿ã‚¤ãƒˆãƒ«: {title}

Abstract: {abstract}

åˆ©ç”¨å¯èƒ½ãªã‚«ãƒ†ã‚´ãƒª:
{json.dumps(category_descriptions, ensure_ascii=False, indent=2)}

ä»¥ä¸‹ã®å½¢å¼ã§JSONã§å›ç­”ã—ã¦ãã ã•ã„ï¼š
{{
    "primary_category": "æœ€ã‚‚é©åˆ‡ãªã‚«ãƒ†ã‚´ãƒªå",
    "confidence_scores": {{"ã‚«ãƒ†ã‚´ãƒª1": 0.85, "ã‚«ãƒ†ã‚´ãƒª2": 0.23}},
    "llm_tags": ["ã‚¿ã‚°1", "ã‚¿ã‚°2", "ã‚¿ã‚°3"],
    "reasoning": "åˆ†é¡ã®ç†ç”±ï¼ˆ1-2æ–‡ï¼‰"
}}

æ³¨æ„: JSONã®å½¢å¼ã‚’å³å¯†ã«å®ˆã£ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚"""
        
        try:
            # æ—¢å­˜ã®ç¿»è¨³æ©Ÿèƒ½ã‚’æ´»ç”¨ã—ã¦LLM APIå‘¼ã³å‡ºã—
            translator = get_translator(self.service)
            response = translator.translate(prompt)
            
            # JSONéƒ¨åˆ†ã‚’æŠ½å‡º
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                return self._validate_classification(result)
            else:
                raise Exception("Could not extract JSON from LLM response")
                
        except Exception as e:
            print(f"LLM classification failed: {e}")
            return self._fallback_classification(title, abstract)
    
    def _validate_classification(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†é¡çµæœã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯"""
        # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ç¢ºèª
        required_fields = ['primary_category', 'confidence_scores', 'llm_tags']
        for field in required_fields:
            if field not in result:
                raise Exception(f"Missing required field: {field}")
        
        # ãƒ—ãƒ©ã‚¤ãƒãƒªã‚«ãƒ†ã‚´ãƒªã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
        if result['primary_category'] not in self.categories:
            raise Exception(f"Invalid primary category: {result['primary_category']}")
        
        # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã®æ­£è¦åŒ–
        confidence_scores = result['confidence_scores']
        total_confidence = sum(confidence_scores.values())
        if total_confidence > 0:
            result['confidence_scores'] = {
                k: v / total_confidence for k, v in confidence_scores.items()
            }
        
        return result
    
    def _fallback_classification(self, title: str, abstract: str) -> Dict[str, Any]:
        """LLMåˆ†é¡å¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ï¼‰"""
        text = f"{title} {abstract}".lower()
        scores = {}
        
        for category, config in self.categories.items():
            score = 0
            for keyword in config['keywords']:
                if keyword.lower() in text:
                    score += 1
            scores[category] = score / len(config['keywords'])
        
        primary_category = max(scores, key=scores.get) if scores else list(self.categories.keys())[0]
        
        return {
            'primary_category': primary_category,
            'confidence_scores': scores,
            'llm_tags': [],
            'reasoning': 'Fallback keyword-based classification'
        }
```

### Phase 2: è¨­å®šã‚·ã‚¹ãƒ†ãƒ æ‹¡å¼µ (Week 3)
**ç›®æ¨™**: æ—¢å­˜è¨­å®šã« Google Drive ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 

#### 2.1 options.html ã®æ‹¡å¼µ
**æ—¢å­˜ã® options.html ã® Google Drive ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æœ‰åŠ¹åŒ–**

```html
<!-- Google Driveé€£æºè¨­å®š -->
<div class="section" id="gdrive-section">
    <h2>â˜ï¸ Google Drive é€£æº</h2>
    
    <div class="checkbox-group">
        <input type="checkbox" id="enableGoogleDrive">
        <label for="enableGoogleDrive">Google Drive è‡ªå‹•æ•´ç†ã‚’æœ‰åŠ¹ã«ã™ã‚‹</label>
    </div>
    <div class="description">ç¿»è¨³å®Œäº†å¾Œã€è‡ªå‹•çš„ã«Google Driveã«ä¿å­˜ã—ã€åˆ†é¡ã—ã¾ã™ã€‚</div>
    
    <div class="form-group gdrive-options" style="display: none;">
        <label for="googleDrivePath">Google Drive ãƒ‘ã‚¹</label>
        <input type="text" id="googleDrivePath" 
               placeholder="Research_Papers">
        <div class="description">Google Driveã«ä½œæˆã™ã‚‹ãƒ•ã‚©ãƒ«ãƒ€åã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚</div>
    </div>
    
    <div class="form-group gdrive-options" style="display: none;">
        <label for="classificationThreshold">åˆ†é¡ä¿¡é ¼åº¦é–¾å€¤</label>
        <input type="range" id="classificationThreshold" 
               min="50" max="95" value="70">
        <span id="thresholdValue">70%</span>
        <div class="description">è‡ªå‹•åˆ†é¡ã®ä¿¡é ¼åº¦é–¾å€¤ã€‚ä½ã„ã»ã©å¤šãã®ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡ã•ã‚Œã¾ã™ã€‚</div>
    </div>
    
    <div class="credentials-section gdrive-options" style="display: none;">
        <h3>ğŸ” èªè¨¼è¨­å®š</h3>
        <button id="authenticateGDrive" class="auth-button">Google Driveèªè¨¼</button>
        <div id="authStatus" class="auth-status"></div>
    </div>
</div>
```

#### 2.2 options.js ã®æ‹¡å¼µ
```javascript
// Google Driveè¨­å®šã®è¿½åŠ 
const EXTENDED_DEFAULT_SETTINGS = {
    ...DEFAULT_SETTINGS,
    enableGoogleDrive: false,
    googleDrivePath: 'Research_Papers',
    classificationThreshold: 70,
    gdriveCredentials: null
};

// Google Driveæœ‰åŠ¹åŒ–ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã®å‡¦ç†
elements.enableGoogleDrive.addEventListener('change', (e) => {
    const gdriveOptions = document.querySelectorAll('.gdrive-options');
    gdriveOptions.forEach(option => {
        option.style.display = e.target.checked ? 'block' : 'none';
    });
});

// èªè¨¼ãƒœã‚¿ãƒ³ã®å‡¦ç†
elements.authenticateGDrive.addEventListener('click', async () => {
    try {
        // Native Hostã«èªè¨¼ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡
        const response = await chrome.runtime.sendMessage({
            action: 'authenticateGoogleDrive'
        });
        
        if (response.success) {
            elements.authStatus.textContent = 'âœ… èªè¨¼æˆåŠŸ';
            elements.authStatus.className = 'auth-status success';
        } else {
            throw new Error(response.error);
        }
    } catch (error) {
        elements.authStatus.textContent = `âŒ èªè¨¼å¤±æ•—: ${error.message}`;
        elements.authStatus.className = 'auth-status error';
    }
});
```

### Phase 3: Native Hostæ‹¡å¼µ (Week 4)
**ç›®æ¨™**: æ—¢å­˜ã®ç¿»è¨³æ©Ÿèƒ½ã‚’ç¶­æŒã—ã¤ã¤ã€Google Driveæ©Ÿèƒ½ã‚’è¿½åŠ 

#### 3.1 pdf2zh_native_host.py ã®æ‹¡å¼µ
```python
# æ—¢å­˜ã®NativeMessagingHostã‚¯ãƒ©ã‚¹ã«ä»¥ä¸‹ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¿½åŠ 

class NativeMessagingHost:
    def __init__(self):
        # æ—¢å­˜ã®åˆæœŸåŒ–
        self.base_dir = Path(__file__).parent
        self.translated_dir = Path("/Users/ytakeda/codes/PDFMathTranslate/translated_pdf")
        self.translated_dir.mkdir(exist_ok=True)
        self.server_port = None
        
        # Google Driveé–¢é€£ã®åˆæœŸåŒ–ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        self.drive_manager = None
        self.abstract_extractor = None
        self.llm_classifier = None
        self._load_gdrive_components()
    
    def _load_gdrive_components(self):
        """Google Driveé–¢é€£ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®é…å»¶èª­ã¿è¾¼ã¿"""
        try:
            from pdf2zh.drive_manager import DriveManager
            from pdf2zh.abstract_extractor import AbstractExtractor
            from pdf2zh.llm_classifier import LLMClassifier
            
            # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ Google Drive è¨­å®šã‚’èª­ã¿è¾¼ã¿
            settings = self._load_extension_settings()
            
            if settings.get('enableGoogleDrive', False):
                credentials_path = os.path.join(self.base_dir, 'gdrive_credentials.json')
                if os.path.exists(credentials_path):
                    self.drive_manager = DriveManager(credentials_path, settings.get('googleDrivePath', 'Research_Papers'))
                    self.abstract_extractor = AbstractExtractor()
                    
                    # ã‚«ãƒ†ã‚´ãƒªè¨­å®šã‚’èª­ã¿è¾¼ã¿
                    categories_config = self._load_categories_config()
                    if categories_config:
                        self.llm_classifier = LLMClassifier(categories_config)
                
        except ImportError as e:
            logger.info(f"Google Drive components not available: {e}")
        except Exception as e:
            logger.warning(f"Failed to initialize Google Drive components: {e}")
    
    def _load_extension_settings(self) -> Dict[str, Any]:
        """Chromeæ‹¡å¼µæ©Ÿèƒ½ã®è¨­å®šã‚’èª­ã¿è¾¼ã¿ï¼ˆå¯èƒ½ãªå ´åˆï¼‰"""
        # å®Ÿè£…ã¯å¾Œã§è©³ç´°åŒ–
        return {}
    
    def _load_categories_config(self) -> Optional[Dict[str, Any]]:
        """ã‚«ãƒ†ã‚´ãƒªè¨­å®šã‚’èª­ã¿è¾¼ã¿"""
        config_path = self.base_dir / 'categories_config.json'
        if config_path.exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        return None
    
    def handle_translate_request(self, message: Dict[str, Any]) -> Dict[str, Any]:
        """ç¿»è¨³ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å‡¦ç†ï¼ˆGoogle Driveçµ±åˆç‰ˆï¼‰"""
        try:
            # æ—¢å­˜ã®ç¿»è¨³å‡¦ç†ã¯å®Œå…¨ã«ç¶­æŒ
            pdf_url = message.get('pdfUrl')
            if not pdf_url:
                raise Exception("PDF URL not provided")
            
            options = message.get('options', {})
            service = options.get('service', 'plamo')
            language = options.get('language', 'ja')
            output_format = options.get('outputFormat', 'dual')
            
            logger.info(f"Processing translation request for: {pdf_url}")
            logger.info(f"Translation options: service={service}, language={language}, format={output_format}")
            
            # PDFã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            pdf_path = self.download_pdf(pdf_url)
            
            try:
                # PDFç¿»è¨³ï¼ˆæ—¢å­˜æ©Ÿèƒ½ï¼‰
                translated_path = self.translate_pdf(pdf_path, service, language, output_format)
                filename = os.path.basename(translated_path)
                
                # ã‚µãƒ¼ãƒãƒ¼èµ·å‹•ï¼ˆæ—¢å­˜æ©Ÿèƒ½ï¼‰
                server_url = self.start_server(translated_path)
                
                # Google Driveçµ±åˆå‡¦ç†ï¼ˆæ–°æ©Ÿèƒ½ã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                gdrive_url = None
                classification_info = None
                
                if self.drive_manager and self.drive_manager.is_available():
                    try:
                        gdrive_url, classification_info = self._process_google_drive_integration(
                            pdf_url, translated_path, service, language, output_format
                        )
                    except Exception as e:
                        logger.warning(f"Google Drive integration failed: {e}")
                
                # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹ç¯‰ï¼ˆæ—¢å­˜ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ã‚’ç¶­æŒï¼‰
                response = {
                    'success': True,
                    'url': server_url,
                    'filename': filename,
                    'message': 'Translation completed successfully'
                }
                
                # Google Driveæƒ…å ±ã‚’è¿½åŠ ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                if gdrive_url:
                    response['gdrive_url'] = gdrive_url
                if classification_info:
                    response['classification'] = classification_info
                
                return response
                
            finally:
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                try:
                    os.remove(pdf_path)
                    os.rmdir(os.path.dirname(pdf_path))
                except:
                    pass
            
        except Exception as e:
            logger.error(f"Translation request failed: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def _process_google_drive_integration(self, pdf_url: str, translated_path: str, 
                                        service: str, language: str, output_format: str) -> tuple:
        """Google Driveçµ±åˆå‡¦ç†"""
        try:
            # arXiv IDã‚’æŠ½å‡º
            arxiv_id = self._extract_arxiv_id(pdf_url)
            
            # Abstractå–å¾—
            metadata = self.abstract_extractor.extract_abstract(arxiv_id, translated_path)
            
            # LLMåˆ†é¡
            if self.llm_classifier:
                classification = self.llm_classifier.classify_paper(
                    metadata.get('title', ''), 
                    metadata.get('abstract', '')
                )
                metadata.update(classification)
            
            # ç¿»è¨³æƒ…å ±ã‚’è¿½åŠ 
            metadata['translation_info'] = {
                'service': service,
                'target_language': language,
                'format': output_format,
                'translated_at': datetime.now().isoformat()
            }
            
            # Google Driveã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            file_id = self.drive_manager.upload_to_papers_folder(translated_path, metadata)
            metadata['drive_file_id'] = file_id
            
            # ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆä½œæˆ
            self._create_category_shortcuts(file_id, metadata)
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
            self._save_paper_metadata(arxiv_id, metadata)
            
            gdrive_url = f"https://drive.google.com/file/d/{file_id}/view"
            classification_info = {
                'primary_category': metadata.get('primary_category'),
                'confidence': metadata.get('confidence_scores', {})
            }
            
            return gdrive_url, classification_info
            
        except Exception as e:
            logger.error(f"Google Drive integration error: {e}")
            raise
    
    def _extract_arxiv_id(self, url: str) -> str:
        """URLã‹ã‚‰arXiv IDã‚’æŠ½å‡º"""
        # arXiv URL pattern matching
        patterns = [
            r'arxiv\.org/abs/(\d{4}\.\d{4,5})',
            r'arxiv\.org/pdf/(\d{4}\.\d{4,5})',
            r'(\d{4}\.\d{4,5})'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æŠ½å‡º
        filename = os.path.basename(url)
        match = re.search(r'(\d{4}\.\d{4,5})', filename)
        if match:
            return match.group(1)
        
        return ""
```

### Phase 4: æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ç§»è¡Œãƒ„ãƒ¼ãƒ« (Week 5-6)
**ç›®æ¨™**: æ—¢å­˜ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–°ã‚·ã‚¹ãƒ†ãƒ ã«ç§»è¡Œ

#### 4.1 Migration Tool
**ãƒ•ã‚¡ã‚¤ãƒ«**: `tools/migrate_existing_papers.py`

```python
#!/usr/bin/env python3
"""
æ—¢å­˜PDFè«–æ–‡ã®ç§»è¡Œãƒ„ãƒ¼ãƒ«
ãƒ­ãƒ¼ã‚«ãƒ«ãƒ»Google Drive ã®æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–°ã‚·ã‚¹ãƒ†ãƒ ã«çµ±åˆ
"""

import os
import json
import sys
from pathlib import Path
from typing import List, Dict, Any
from pdf2zh.drive_manager import DriveManager
from pdf2zh.abstract_extractor import AbstractExtractor
from pdf2zh.llm_classifier import LLMClassifier

class PaperMigrator:
    def __init__(self, local_dir: str, drive_credentials_path: str):
        self.local_dir = Path(local_dir)
        self.drive_manager = DriveManager(drive_credentials_path)
        self.abstract_extractor = AbstractExtractor()
        self.llm_classifier = None
        self.processed_papers = []
        self.error_log = []
    
    def inventory_existing_files(self) -> List[Dict[str, Any]]:
        """æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®æ£šå¸ã—"""
        print("ğŸ“‹ Inventorying existing files...")
        inventory = []
        
        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        local_files = list(self.local_dir.glob("*-dual.pdf")) + list(self.local_dir.glob("*-mono.pdf"))
        
        for local_file in local_files:
            arxiv_id = self._extract_arxiv_id_from_filename(local_file.name)
            
            inventory.append({
                "arxiv_id": arxiv_id,
                "local_path": str(local_file),
                "filename": local_file.name,
                "size": local_file.stat().st_size,
                "needs_processing": True
            })
        
        print(f"ğŸ“Š Found {len(inventory)} local files")
        return inventory
    
    def migrate_batch(self, inventory: List[Dict[str, Any]], batch_size: int = 10) -> None:
        """ãƒãƒƒãƒå˜ä½ã§ãƒ•ã‚¡ã‚¤ãƒ«ç§»è¡Œ"""
        print(f"ğŸš€ Starting migration of {len(inventory)} files...")
        
        for i in range(0, len(inventory), batch_size):
            batch = inventory[i:i+batch_size]
            print(f"\nğŸ“¦ Processing batch {i//batch_size + 1}/{(len(inventory)-1)//batch_size + 1}")
            
            for item in batch:
                try:
                    self._migrate_single_paper(item)
                    self.processed_papers.append(item)
                    print(f"âœ… {item['arxiv_id']}")
                    
                except Exception as e:
                    error_info = {
                        'arxiv_id': item['arxiv_id'],
                        'error': str(e),
                        'file': item['local_path']
                    }
                    self.error_log.append(error_info)
                    print(f"âŒ {item['arxiv_id']}: {e}")
        
        self._generate_migration_report()
    
    def _migrate_single_paper(self, item: Dict[str, Any]) -> None:
        """å˜ä¸€è«–æ–‡ã®ç§»è¡Œå‡¦ç†"""
        arxiv_id = item['arxiv_id']
        local_path = item['local_path']
        
        # 1. Abstractå–å¾—
        try:
            metadata = self.abstract_extractor.extract_abstract(arxiv_id, local_path)
        except:
            # Abstractå–å¾—å¤±æ•—æ™‚ã¯åŸºæœ¬æƒ…å ±ã®ã¿
            metadata = {
                'title': f"Paper {arxiv_id}",
                'abstract': '',
                'authors': [],
                'source': 'migration'
            }
        
        # 2. LLMåˆ†é¡ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if self.llm_classifier and metadata.get('abstract'):
            try:
                classification = self.llm_classifier.classify_paper(
                    metadata['title'], metadata['abstract']
                )
                metadata.update(classification)
            except:
                metadata.update({
                    'primary_category': 'Others',
                    'confidence_scores': {},
                    'llm_tags': []
                })
        
        # 3. Google Driveã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        file_id = self.drive_manager.upload_to_papers_folder(local_path, metadata)
        metadata['drive_file_id'] = file_id
        
        # 4. ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆä½œæˆ
        if metadata.get('primary_category'):
            self._create_migration_shortcuts(file_id, metadata)
        
        # 5. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        self._save_paper_metadata(arxiv_id, metadata)
    
    def _generate_migration_report(self) -> None:
        """ç§»è¡Œãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report = {
            'migration_date': datetime.now().isoformat(),
            'total_files': len(self.processed_papers) + len(self.error_log),
            'successful': len(self.processed_papers),
            'failed': len(self.error_log),
            'processed_papers': self.processed_papers,
            'errors': self.error_log
        }
        
        report_path = self.local_dir / 'migration_report.json'
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“Š Migration Report:")
        print(f"   âœ… Successful: {report['successful']}")
        print(f"   âŒ Failed: {report['failed']}")
        print(f"   ğŸ“„ Report saved: {report_path}")

if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("Usage: python migrate_existing_papers.py <local_dir> <credentials_path>")
        sys.exit(1)
    
    migrator = PaperMigrator(sys.argv[1], sys.argv[2])
    inventory = migrator.inventory_existing_files()
    migrator.migrate_batch(inventory)
```

### Phase 5: ãƒ†ã‚¹ãƒˆã¨ãƒ‡ãƒ—ãƒ­ã‚¤ (Week 7-8)

#### 5.1 å˜ä½“ãƒ†ã‚¹ãƒˆã®ä½œæˆ
**ãƒ•ã‚¡ã‚¤ãƒ«**: `test/test_google_drive_integration.py`

```python
#!/usr/bin/env python3
"""
Google Driveçµ±åˆæ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ
"""

import unittest
from unittest.mock import Mock, patch, MagicMock
from pdf2zh.abstract_extractor import AbstractExtractor
from pdf2zh.llm_classifier import LLMClassifier
from pdf2zh.drive_manager import DriveManager

class TestAbstractExtractor(unittest.TestCase):
    def setUp(self):
        self.extractor = AbstractExtractor()
    
    def test_arxiv_id_detection(self):
        """arXiv IDæ¤œå‡ºã®ãƒ†ã‚¹ãƒˆ"""
        self.assertTrue(self.extractor.is_arxiv_id("2506.06751"))
        self.assertTrue(self.extractor.is_arxiv_id("1234.5678"))
        self.assertFalse(self.extractor.is_arxiv_id("invalid"))
    
    @patch('arxiv.Client')
    def test_arxiv_extraction(self, mock_client):
        """arXiv APIæŠ½å‡ºã®ãƒ†ã‚¹ãƒˆ"""
        # Mockè¨­å®š
        mock_paper = Mock()
        mock_paper.title = "Test Paper"
        mock_paper.summary = "Test abstract"
        mock_paper.authors = [Mock(name="John Doe")]
        mock_paper.entry_id = "https://arxiv.org/abs/2506.06751"
        mock_paper.published = Mock()
        mock_paper.published.isoformat.return_value = "2025-06-12T10:00:00Z"
        
        mock_client.return_value.results.return_value = [mock_paper]
        
        result = self.extractor.extract_from_arxiv("2506.06751")
        
        self.assertEqual(result['title'], "Test Paper")
        self.assertEqual(result['abstract'], "Test abstract")
        self.assertEqual(result['source'], "arxiv_api")

class TestLLMClassifier(unittest.TestCase):
    def setUp(self):
        categories_config = {
            'categories': {
                'ML_AI': {
                    'name': 'æ©Ÿæ¢°å­¦ç¿’ãƒ»AI',
                    'keywords': ['machine learning', 'deep learning']
                }
            },
            'classification_rules': {
                'confidence_threshold': 0.7
            }
        }
        self.classifier = LLMClassifier(categories_config)
    
    def test_fallback_classification(self):
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†é¡ã®ãƒ†ã‚¹ãƒˆ"""
        result = self.classifier._fallback_classification(
            "Deep Learning for NLP", 
            "This paper presents a deep learning approach..."
        )
        
        self.assertIn('primary_category', result)
        self.assertIn('confidence_scores', result)

if __name__ == '__main__':
    unittest.main()
```

## ğŸš€ ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæˆ¦ç•¥

### æ®µéšçš„ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆ

1. **Phase 1-2**: åŸºç›¤ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºãƒ»ãƒ†ã‚¹ãƒˆ
2. **Phase 3**: è¨­å®šç”»é¢ã§ã® Google Drive ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¿½åŠ ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç„¡åŠ¹ï¼‰
3. **Phase 4**: Native Host ã§ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³æ©Ÿèƒ½çµ±åˆ
4. **Phase 5**: æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ç§»è¡Œãƒ„ãƒ¼ãƒ«ã®æä¾›
5. **Phase 6**: æœ¬æ ¼é‹ç”¨é–‹å§‹

### ãƒªã‚¹ã‚¯è»½æ¸›ç­–

- **æ©Ÿèƒ½ãƒ•ãƒ©ã‚°**: å…¨ã¦ã®æ–°æ©Ÿèƒ½ã¯è¨­å®šã§ç„¡åŠ¹åŒ–å¯èƒ½
- **ä¾‹å¤–å‡¦ç†**: Google Driveæ©Ÿèƒ½ã®å¤±æ•—ã¯æ—¢å­˜æ©Ÿèƒ½ã«å½±éŸ¿ã—ãªã„
- **ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—**: ç§»è¡Œå‰ã«æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®å®Œå…¨ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
- **æ®µéšçš„ç§»è¡Œ**: å°è¦æ¨¡ãƒ†ã‚¹ãƒˆã‹ã‚‰æœ¬æ ¼ç§»è¡Œã¸

## ğŸ“‹ å¿…è¦ãªæº–å‚™ä½œæ¥­

### é–‹ç™ºç’°å¢ƒ

```bash
# å¿…è¦ãªPythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install arxiv pdfplumber google-api-python-client google-auth

# Google Drive APIèªè¨¼è¨­å®š
# 1. Google Cloud Consoleã§ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
# 2. Drive APIæœ‰åŠ¹åŒ–
# 3. OAuth2èªè¨¼æƒ…å ±ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
```

### è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«

1. **Google Drive APIèªè¨¼**: `gdrive_credentials.json`
2. **ã‚«ãƒ†ã‚´ãƒªè¨­å®š**: `categories_config.json`
3. **æ‹¡å¼µæ©Ÿèƒ½è¨­å®š**: Chrome storage sync

## âš ï¸ é‡è¦ãªæ³¨æ„äº‹é …

- **ç¾åœ¨ã®æ‹¡å¼µæ©Ÿèƒ½ã¯ä¸€åˆ‡å¤‰æ›´ã—ãªã„**: æ–°æ©Ÿèƒ½ã¯ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¨ã—ã¦è¿½åŠ 
- **ãƒ‡ãƒ¼ã‚¿æå¤±é˜²æ­¢**: ç§»è¡Œå‰ã«å¿…ãšãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆ
- **æ®µéšçš„ãƒ†ã‚¹ãƒˆ**: å°è¦æ¨¡ãƒ†ã‚¹ãƒˆã‹ã‚‰é–‹å§‹ã—ã€å•é¡ŒãŒãªã„ã“ã¨ã‚’ç¢ºèª
- **ãƒ¦ãƒ¼ã‚¶ãƒ¼åŒæ„**: Google Driveæ©Ÿèƒ½ã¯æ˜ç¤ºçš„ã«æœ‰åŠ¹åŒ–ãŒå¿…è¦

---

**ã“ã®å®Ÿè£…è¨ˆç”»ã¯ã€ç¾åœ¨ã® PDF Math Translate Chrome Extension ã®æ©Ÿèƒ½ã‚’å®Œå…¨ã«ä¿æŒã—ã¤ã¤ã€æ–°ã—ã„ Google Drive çµ±åˆæ©Ÿèƒ½ã‚’æ®µéšçš„ã«è¿½åŠ ã™ã‚‹ãŸã‚ã®è©³ç´°ã‚¬ã‚¤ãƒ‰ã§ã™ã€‚å„ãƒ•ã‚§ãƒ¼ã‚ºã¯ç‹¬ç«‹ã—ã¦å®Ÿè£…å¯èƒ½ã§ã€æ—¢å­˜ã®ç¿»è¨³ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã«å½±éŸ¿ã‚’ä¸ãˆã¾ã›ã‚“ã€‚**